name: Voicy Episodes Scraper

on:
  schedule:
    # 毎週日曜日の午前2時に実行 (UTC)
    - cron: '0 2 * * 0'
  workflow_dispatch:  # 手動実行用

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: チェックアウト
        uses: actions/checkout@v3
      
      - name: ファイル一覧確認（デバッグ用）
        run: |
          pwd
          echo "リポジトリのルートディレクトリ:"
          ls -la
          echo "---------------------------------------"
          find . -type f -name "*.py" | sort
      
      - name: Pythonスクリプトの作成
        run: |
          cat > scraper.py << 'EOL'
          import requests
          from bs4 import BeautifulSoup
          import xml.etree.ElementTree as ET
          from xml.dom import minidom
          import time
          import os
          import logging
          from datetime import datetime

          # ロギングの設定
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger(__name__)

          # 設定
          CHANNEL_ID = "2834"
          BASE_URL = f"https://voicy.jp/channel/{CHANNEL_ID}/all"
          OUTPUT_FILE = "voicy_episodes.xml"
          HEADERS = {
              "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
          }

          def prettify_xml(elem):
              """XMLを整形して返す"""
              rough_string = ET.tostring(elem, 'utf-8')
              reparsed = minidom.parseString(rough_string)
              return reparsed.toprettyxml(indent="  ")

          def get_page_content(url):
              """指定URLのページ内容を取得"""
              try:
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  return response.text
              except requests.RequestException as e:
                  logger.error(f"ページの取得に失敗しました: {url}, エラー: {e}")
                  return None

          def parse_episode_data(url):
              """エピソードの詳細ページからデータを抽出"""
              content = get_page_content(url)
              if not content:
                  return None
              
              soup = BeautifulSoup(content, 'html.parser')
              
              try:
                  # タイトル
                  title = soup.select_one('h1.voice-title').text.strip()
                  
                  # プレミアム限定か無料配信か
                  status_elem = soup.select_one('span.premium-icon')
                  status = "プレミアム限定" if status_elem else "無料配信"
                  
                  # 配信日時
                  date_elem = soup.select_one('div.voice-detail div.date')
                  date_str = date_elem.text.strip() if date_elem else "不明"
                  
                  # 配信分数
                  duration_elem = soup.select_one('div.voice-detail div.duration')
                  duration = duration_elem.text.strip() if duration_elem else "不明"
                  
                  # タグ
                  tags_elems = soup.select('div.tag span')
                  tags = [tag.text.strip() for tag in tags_elems]
                  
                  return {
                      "url": url,
                      "title": title,
                      "status": status,
                      "date": date_str,
                      "duration": duration,
                      "tags": tags
                  }
              except Exception as e:
                  logger.error(f"エピソードデータの解析に失敗しました: {url}, エラー: {e}")
                  return None

          def get_all_episodes():
              """チャンネルの全エピソードのURLを取得"""
              page_num = 1
              all_episode_urls = []
              
              while True:
                  logger.info(f"ページ {page_num} を取得中...")
                  if page_num == 1:
                      url = BASE_URL
                  else:
                      url = f"{BASE_URL}?page={page_num}"
                  
                  content = get_page_content(url)
                  if not content:
                      break
                  
                  soup = BeautifulSoup(content, 'html.parser')
                  episode_links = soup.select('a.voice-list-item')
                  
                  if not episode_links:
                      break
                  
                  for link in episode_links:
                      episode_url = "https://voicy.jp" + link['href']
                      all_episode_urls.append(episode_url)
                  
                  # 次のページがあるか確認
                  next_button = soup.select_one('li.next a')
                  if not next_button:
                      break
                  
                  page_num += 1
                  time.sleep(1)  # サーバーに負荷をかけないよう少し待機
              
              return all_episode_urls

          def create_xml(episodes):
              """エピソードデータからXMLを作成"""
              root = ET.Element("voicy_episodes")
              root.set("channel_id", CHANNEL_ID)
              root.set("generated_at", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
              
              for episode in episodes:
                  if not episode:
                      continue
                  
                  episode_elem = ET.SubElement(root, "episode")
                  
                  url_elem = ET.SubElement(episode_elem, "url")
                  url_elem.text = episode["url"]
                  
                  title_elem = ET.SubElement(episode_elem, "title")
                  title_elem.text = episode["title"]
                  
                  status_elem = ET.SubElement(episode_elem, "status")
                  status_elem.text = episode["status"]
                  
                  date_elem = ET.SubElement(episode_elem, "date")
                  date_elem.text = episode["date"]
                  
                  duration_elem = ET.SubElement(episode_elem, "duration")
                  duration_elem.text = episode["duration"]
                  
                  tags_elem = ET.SubElement(episode_elem, "tags")
                  for tag in episode["tags"]:
                      tag_elem = ET.SubElement(tags_elem, "tag")
                      tag_elem.text = tag
              
              return root

          def main():
              """メイン処理"""
              try:
                  logger.info("Voicyエピソードの取得を開始します")
                  
                  # 全エピソードのURLを取得
                  episode_urls = get_all_episodes()
                  logger.info(f"合計 {len(episode_urls)} のエピソードを見つけました")
                  
                  # 古い順にソート (URLの数字部分でソート)
                  episode_urls.sort(key=lambda url: int(url.split('/')[-1]))
                  
                  # 各エピソードの詳細を取得
                  episodes_data = []
                  for i, url in enumerate(episode_urls):
                      logger.info(f"エピソード {i+1}/{len(episode_urls)} を処理中: {url}")
                      episode_data = parse_episode_data(url)
                      if episode_data:
                          episodes_data.append(episode_data)
                      time.sleep(1)  # サーバーに負荷をかけないよう少し待機
                  
                  # XMLを作成
                  xml_root = create_xml(episodes_data)
                  xml_str = prettify_xml(xml_root)
                  
                  # XMLをファイルに保存
                  with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                      f.write(xml_str)
                  
                  logger.info(f"XMLファイルを保存しました: {OUTPUT_FILE}")
              
              except Exception as e:
                  logger.error(f"予期せぬエラーが発生しました: {e}")

          if __name__ == "__main__":
              main()
          EOL
          
          echo "スクリプトファイルを作成しました"
          ls -la
      
      - name: Python環境のセットアップ
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: 依存関係のインストール
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4
      
      - name: スクレイピングの実行
        run: |
          python scraper.py
          ls -la
      
      - name: 変更のコミット
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add voicy_episodes.xml
          git diff --quiet && git diff --staged --quiet || git commit -m "エピソードデータを更新 $(date '+%Y-%m-%d')"
      
      - name: リモートにプッシュ
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref }}
